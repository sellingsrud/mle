{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "#  Exercise Set 1: Introduction to modeling and machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementing the following classifiers:\n",
    "- Perceptron\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Superclass for classifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta=0.1, N=100, w=None):\n",
    "        if eta <= 0 or eta > 1:\n",
    "            raise ValueError(\"Learning rate eta must be between 0 and 1\")\n",
    "\n",
    "        self.eta = eta\n",
    "        self.N = N\n",
    "        self.w = w\n",
    "        self.errors = []\n",
    "\n",
    "    def z(self, x):\n",
    "        \"\"\"\n",
    "        Weighted sum of x by w.\n",
    "        \"\"\"\n",
    "        return self.w.T @ x\n",
    "\n",
    "\n",
    "class Perceptron(Classifier):\n",
    "    \"\"\"\n",
    "    The Perceptron classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def phi(self, x):\n",
    "        return 1 if self.z(x) > 0 else -1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        nrows, ncols = X.shape\n",
    "\n",
    "        # Add a constant as the first column\n",
    "        x0 = np.ones(shape=(nrows, 1))\n",
    "        X = np.hstack((x0, X))\n",
    "\n",
    "        # Initialize weights if not given\n",
    "        # Add 1 to size to account for the ones added above\n",
    "        if self.w is None:\n",
    "            self.w = np.random.normal(loc=0, scale=0.01, size=ncols + 1)\n",
    "\n",
    "        for _ in range(self.N):\n",
    "            error = y - np.array([self.phi(x) for x in X])\n",
    "            dw = self.eta * error @ X\n",
    "            self.w += dw.flatten()\n",
    "            self.errors.append(np.where(error != 0, 1, 0).sum())\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.z(X) >= 0.0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92783381  1.87743943  5.23739755 -7.15240778 -3.05113605]\n",
      "[50, 50, 50, 50, 50, 50, 50, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "names = [\"Class\"] + iris.feature_names\n",
    "df = pd.DataFrame(np.hstack((iris.target.reshape(len(iris.data), 1), iris.data)), columns = names)\n",
    "df = df[df.Class <= 1]\n",
    "df.Class = df.Class.apply(lambda x: 1 if x == 0.0 else -1)\n",
    "\n",
    "p = Perceptron(eta = 0.01, N = 25)\n",
    "p.fit(df.drop(\"Class\", axis = 1).to_numpy(), df.Class.to_numpy())\n",
    "print(p.w)\n",
    "print(p.errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond the perceptron model\n",
    "\n",
    "Having seen and worked with the perceptron I want to provide you with some ideas on how we can change parts of the perceptron to obtain another model. Again, you may want to familiarize yourself with background concepts: [gradient](https://en.wikipedia.org/wiki/Gradient), [sum of squared errors](https://en.wikipedia.org/wiki/Residual_sum_of_squares) and the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is another simple linear machine-learning algorithm, you can read about it [here:](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Ex. 1.2.1:** Import the LogisticRegression classifier from `sklearn.linear_model`. Create a new object called `clf` like:\n",
    "```\n",
    "clf = LogisticRegression()\n",
    "```\n",
    "All scikit learn models have two fundamental methods `.fit()` and `.predict()`. Fit your model to the training data, and store the fitted model in a new object. Import _accuracy_score_ from `sklearn.metrics` and asses the accuracy of the LogisticRegression on both your training data and your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "names = [\"Class\"] + iris.feature_names\n",
    "df = pd.DataFrame(np.hstack((iris.target.reshape(len(iris.data), 1), iris.data)), columns = names)\n",
    "df = df[df.Class <= 1]\n",
    "df.Class = df.Class.apply(lambda x: 1 if x == 0.0 else -1)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "X = df.drop(\"Class\", axis = 1).to_numpy()\n",
    "y = df.Class.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "\n",
    "fitted = clf.fit(X_train, y_train)\n",
    "ypred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, ypred))\n",
    "print(accuracy_score(y_test, ypred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression mechanics\n",
    "### Implementing and evaluating the gradient decent \n",
    " \n",
    "Normally we use OLS to estimate linear regression models, but this is only way of solving the problem of minimizing the least squares problem (that minimizes the sum of squared errors). \n",
    "\n",
    "You may find PML pp. 310-312, 319-324 useful as background reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue straight to an exercise where you are to implement a new estimator that we code up from scratch. We solve the numerical optimization using the gradient decent algorithm. Using our algorithm we will fit it to some data, and compare our own solution to the standard solution from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.0**: Import the dataset `tips` from the `seaborn`.\n",
    "\n",
    "\n",
    "*Hint*: use the `load_dataset` method in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"tips\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.1**: Convert non-numeric variables to dummy variables for each category (remember to leave one column out for each catagorical variable, so you have a reference). Restructure the data so we get a dataset `y` containing the variable tip, and a dataset `X` containing the \n",
    "features. \n",
    "\n",
    ">> *Hint*: You might want to use the `get_dummies` method in pandas, with the `drop_first = True` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first = True)\n",
    "target = \"tip\"\n",
    "X = df.drop(target, axis = 1)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.2**: Divide the features and target into test and train data. Make the split 50 pct. of each. The split data should be called `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    ">> *Hint*: You may use `train_test_split` in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.3**: Normalize your features by converting to zero mean and one std. deviation.\n",
    "\n",
    ">> *Hint 1*: Take a look at `StandardScaler` in `sklearn.preprocessing`. \n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StandardScaler()\n",
    "X_train_prep = s.fit_transform(X_train)\n",
    "X_test_prep = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.4**: Make a function called `compute_error` to compute the prediction errors given input target `y_`, input features `X_` and input weights `w_`. You should use matrix multiplication.\n",
    ">\n",
    ">> *Hint 1:* You can use the net-input fct. from yesterday.\n",
    ">>\n",
    ">> *Hint 2:* If you run the following code,\n",
    ">> ```python\n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "compute_error(y__, X__, w__)\n",
    "```\n",
    "\n",
    ">> then you should get output:\n",
    "```python \n",
    "array([0,0])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_error(y, X, w):\n",
    "    return y - (w[0] + X @ w[1:])\n",
    "\n",
    "# Test\n",
    "\n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "e = compute_error(y__, X__, w__)\n",
    "assert np.all((e == 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.5**: Make a function to update the weights given input target `y_`, input features `X_` and input weights `w_` as well as learning rate, $\\eta$, i.e. greek `eta`. You should use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(y, X, w, eta = 0.001):\n",
    "    y = np.reshape(y, (X.shape[0], 1))\n",
    "    error = y - (w[0] + X @ w[1:])\n",
    "    w[0] += eta * np.sum(error)\n",
    "    w[1:] += eta * (X.T @ error)\n",
    "    return w, error\n",
    "\n",
    "w = np.zeros(1 + X.shape[1]).reshape((5, 1))\n",
    "w, error = update(y_train, X_train, w, eta = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.6**: Use the code below to initialize weights `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17278735.41312099]\n",
      " [96639124.79532792]\n",
      " [53504322.09872471]\n",
      " [53872766.05374784]\n",
      " [15208800.45602682]]\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros(1 + X.shape[1]).reshape((5, 1))\n",
    "\n",
    "for i in range(50):\n",
    "    w, err = update(y_train, X_train, w)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.7**: Make a function to compute the mean squared error. Alter the loop so it makes 100 iterations and computes the MSE for test and train after each iteration, plot these in one figure. \n",
    "\n",
    ">> Hint: You can use the following code to check that your model works:\n",
    ">>```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "assert((w[1:] - reg.coef_).sum() < 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 11.1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bonus exercises are for those who have completed all other exercises until now and have a deep motivation for learning more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 11.1.8 (BONUS)**: Implement your linear regression model as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ANSWER: A solution is found on p. 320 in Python for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaLine (BONUS)\n",
    "AdaLine is a modified version of the perceptron. The only difference lies in the way the two models learn from their training data, i.e. the optimization method used. The perceptron used the binary classifications for learning, while AdaLine only applies the binary threshold after training, and thus uses real valued numbers when learning. \n",
    ">> _Hint:_ Most of the code for this exercise can be written by copying and modifying code from exercise 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.3.1 (BONUS):** Implement two functions described below. You shold reuse your `net_input` from Ex. 1.1.4.:\n",
    "* `ada_activation_function`: the identify function $ada\\_activation(z) = z$\n",
    "* `ada_predict`: A step function   $ada\\_predict(z) = 1 \\ if \\ z \\geq 0  \\ else \\ 0$ where z is the output of _the activation function_.\n",
    "\n",
    "\n",
    "\n",
    "> The following figure might help you understand how each of these functions relate to the algorithm, and how the perceptron and adaline differ:\n",
    "![asd](https://sebastianraschka.com/images/faq/diff-perceptron-adaline-neuralnet/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 1.3.1 BONUS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.3.2 (BONUS):** AdaLine uses a _cost function_ to quantize the accuracy of the classifier this is given by \n",
    ">$$ \n",
    "cost(X,y,W) = \\frac{1}{2} \\sum_{i=1}^N (y_i - activation(z_i) )^2 , \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    "> If you've followed any normal undergraduate courses in statistics you should recognize this function. Begin by implementing the cost function. Unlike in undergraduate statistics we will optimize our estimator using gradient descent, therefore **code up the negative of the derivative of the cost function as well**. \n",
    "> $$ \n",
    "-cost'_j(X,y, W) = -\\sum_{i=1}^N (y_i - activation(z_i)) x_i^j,  \\qquad z_i = net\\_input(x_i, W)\n",
    "$$\n",
    ">\n",
    ">> _Hint:_ Dont compute the sum for each weight $w_j$, instead use numpy's matrix algebra to compute the all of the derivatives at once.\n",
    ">\n",
    ">> _Hint:_ The derivative should return a list of the same length as the number of weights, since there is one derivative for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 1.3.2 BONUS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.3.3 BONUS:** Implement the adaline fitting algorithm using *batch gradient descent*. This is similar to what you did with the perceptron, but while the perceptron did it's optimization after evaluating each row in the dataset, adaline treats the entire dataset as a batch, adjusts it's weights and then does it all again. Thus you only need to loop over `n_iter`, _not_ the data rows. Use the cost function to track the progress of your algorithm.\n",
    ">\n",
    ">> _Hint:_ gradient descent will be extremely sensitive to the learning rate $\\eta$ in this situation - try setting i to 0.0001 and running the algorithm for 5000 iterations to get some kind of convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.3.3 BONUS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.3.4 (BONUS):** Write a function that scales each of the variables in the dataset (including **y**) using the formula \n",
    "$$\n",
    "x_j^{new} = \\frac{x_j^{old} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "> rerun the adaline function on the scaled variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 1.3.4 BONUS]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
