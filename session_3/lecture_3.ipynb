{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recording of class\n",
    "Background: people join from Americas/East Asia\n",
    "\n",
    "Solution: recording of class\n",
    "- Will be put on secret youtube links - not searchable\n",
    "- Delete mid June\n",
    "\n",
    "Consent: via poll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3:\n",
    "## Non-linear ML\n",
    "\n",
    "- A tour of the most essential supervised models\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Universal Approximation\n",
    "\n",
    "Definition: Given enough data the algorithm comes as close to sampling distribution as possible\n",
    "- Note: does not care about causation or selection! Purely prediction\n",
    "    \n",
    "Examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can also make input non-linear using `PolynomialFeatures` of any order.\n",
    "  - Follows from iterative Taylor expansion\n",
    "  - Problem: spurious coefficients and large coefficients\n",
    "- Others approaches? non-linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "More supervised tools\n",
    "\n",
    "1. [Measuring classification performance](#Measuring-classification-performance)\n",
    "1. [Nested cross validation](#Nested-cross-validation)\n",
    "\n",
    "Non-linear ML models\n",
    "\n",
    "1. [Kernel-based models](#Kernel-based-models)\n",
    "1. [Tree-based modelling](#Tree-based-models)\n",
    "1. [Ensemble learning](#Ensemble-learning)\n",
    "1. [Neural networks and deep learning](#Neural-networks-and-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data for this lecture\n",
    "\n",
    "We begin by loading the titanic dataset, *survived* is target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = sns.load_dataset('titanic')\\\n",
    "        .assign(male=lambda df: df.sex=='male',\n",
    "                age_null = lambda df: df.age.isnull())\n",
    "drop_cols = ['survived','adult_male','class','who','alive','embark_town','sex', 'deck']\n",
    "y,X = df['survived'], pd.get_dummies(df.drop(drop_cols, axis=1), dummy_na=True, drop_first=True).fillna(-99)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "auc_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring classification performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (1)\n",
    "\n",
    "We measure the accaracy as the rate of true predictions, i.e. \\begin{align}ACC&=\\frac{True}{True+False}\\end{align}\n",
    "\n",
    "Can we decompose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (2)\n",
    "Yes, we can decompose into\n",
    "$$ACC=\\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_08.png' alt=\"Drawing\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (3)\n",
    "\n",
    "Some powerful measures:\n",
    "\n",
    "- Precision: share of *predicted positive* that are true\n",
    "    - PRE = $\\frac{TP}{TP+FP}$    \n",
    "    - = true positive rate \n",
    "- Recall: share of *actual positive* that are true    \n",
    "   - REC = $\\frac{TP}{TP+FN}=\\frac{TP}{AP}$ \n",
    "   - = 1- false negative rate\n",
    "- F1: mix recall and precision: $\\frac{2\\cdot PRE\\cdot REC}{PRE+ REC}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (4)\n",
    "\n",
    "Classification models provide a predicted likelihood of being in the class or not:\n",
    "- Receiver Operating Characteristic (ROC) curve by varying thresholds for predicted true.\n",
    "    - ROC is a *theoretical* measure of model performance based on probabilities.\n",
    "    - AUC: Area Under the (ROC) Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Breakdown by error type (5)\n",
    "\n",
    "Example of Area Under the (ROC) Curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_10.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nested cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model test does not consider uncertainty from fact that we are also tuning hyperparameters:\n",
    "  - Leads too overfitting (Varma & Simon 2006; Cawley, Talbot 2010).\n",
    "- Solution is **nested cross validation**.\n",
    "  - Validation step should not be modelled as 1) train; 2) test.\n",
    "  - Better way is 1) model selection: train, validate; 2) test.\n",
    "  - Implement as pp 204-205 in Python for Machine Learning:\n",
    "      - first inner loop: `GridSearchCV` \n",
    "      - second outer loop: `cross_val_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (2)\n",
    "A depiction of the process:\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_07.png' alt=\"Drawing\" style=\"width: 450px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (3)\n",
    "Example of application: [Bjerre-Nielsen et al. (2021, PNAS)](https://doi.org/10.1073/pnas.2020258118)\n",
    "  - Show that student surveillance using phones does not need to better models of academic performance\n",
    "    - Important for algorithmic policy making!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel-based models\n",
    "\n",
    "### *Learn from others like you*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels\n",
    "\n",
    "What is a kernel? A mapping $k$ that computes *similarity* between two vectors. \n",
    "\n",
    "- E.g. how similar are the two observations $x_i$ and $x_j$ from space $\\mathcal{X}$? \n",
    "- Formally a kernel is a mapping $k: \\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$\n",
    "\n",
    "How can we use it as a supervised model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let be a binary target, $y\\in\\{1,-1\\}$. \n",
    "\n",
    "\n",
    "We can use a kernel to weight observations from training data-set:\n",
    "\n",
    "\n",
    "$${\\hat {y}_i}=\\operatorname {sgn} \\sum _{j=1}^{n}y_{j}k(x _{i},x_j )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels\n",
    "\n",
    "One example of a kernel is the **k-nearest neighbor**. \n",
    "\n",
    "1. Compute $||x_i-x_j||$, i.e. distance norm between $x_i$ and all $x_j$, e.g. Euclidian distance ($L_2$ norm)\n",
    "2. The kernel $k(x_i,x_j)$ equals $1/w$ if it's one of the $k$ smallest distances; otherwise $k(x_i,x_j)=0$.\n",
    "\n",
    "Many alternative kernels:\n",
    "- Radius neighbors, which weights all observations within a threshold $r$ evenly, i.e. if $k(x_i,x_j)<r$.\n",
    "- E.g. polynomial, radial basis function and many more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of kernels\n",
    "\n",
    "\n",
    "We can use KNN to aggregate over the $k$ most similar observations. \n",
    "- The KNN classifier computes the mode\n",
    "- The KNN regressor computes the mean\n",
    "- Note neighbor kernel are common econometrics too \n",
    "\n",
    "Generally, we may use the Nadaraya-Watson estimator using for kernel $k$ (aka. kernel regression)\n",
    "\n",
    "$$\\widehat{y}_{k}(x)={\\frac {\\sum _{i=1}^{n}k(x-x_{i})y_{i}}{\\sum _{i=1}^{n}k(x-x_{i})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5_nearest_neighbors': 0.737}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_dt = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_dt.fit(X_train,y_train)\n",
    "auc_scores['5_nearest_neighbors'] = round(roc_auc_score(y_test, clf_dt.predict_proba(X_test)[:, 1]), 3)\n",
    "auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of kernels\n",
    "\n",
    "\n",
    "Example of the k-nearest neighbor classifier where the binary values are illustrated as red (= -1) / blue (= 1). \n",
    "\n",
    "\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/419px-KnnClassification.svg.png' alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "\n",
    "*(artist: [Antti Ajanki](https://commons.wikimedia.org/wiki/File:KnnClassification.svg), CC-SA 1.0, no changes)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of kernels\n",
    "\n",
    "\n",
    "Another  application is: **Support Vector Machine** (SVM) is like logistic regression (separates green/blue):\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/926px-SVM_margin.png' alt=\"Drawing\" style=\"width: 450px;\"/></center>\n",
    "\n",
    "*(artist: [Larhmam](https://commons.wikimedia.org/wiki/File:SVM_margin.png), CC-SA 4.0, no changes)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application of kernels\n",
    "\n",
    "SVMs have two major advantages over logistic regression\n",
    "- Computational efficiency (optimization under constraints)\n",
    "- Has non-linear implementation kernels that are decomposable by inner product space, i.e. $k(x_i,x_j)=\\left\\langle\\varphi(x_i),\\varphi(x_j) \\right\\rangle$\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/d/d8/Kernel_yontemi_ile_veriyi_daha_fazla_dimensiyonlu_uzaya_tasima_islemi.png\n",
    "' alt=\"Drawing\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "\n",
    "\n",
    "*(artist: [Shehzadex](https://commons.wikimedia.org/wiki/File:Kernel_yontemi_ile_veriyi_daha_fazla_dimensiyonlu_uzaya_tasima_islemi.png), CC-SA 4.0, no changes)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree-based models\n",
    "\n",
    "### *Divide and conquer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Situation - we want to predict who will become a criminal\n",
    "<center><img src='fig/decision_tree/0001.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "How can we make a tree to determine who becomes criminal?\n",
    "<center><img src='fig/decision_tree/0002.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Situation - want to predict who will become a criminal\n",
    "<center><img src='fig/decision_tree/0003.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Split 1: by place of birth\n",
    "<center><img src='fig/decision_tree/0004.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Split 2: by alcoholic mother\n",
    "<center><img src='fig/decision_tree/0005.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Alternative split 1: by alcoholic mother\n",
    "<center><img src='fig/decision_tree/0006.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "How can we automate the splitting process of data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, we can do as follows:\n",
    "\n",
    "- Use measure/criterion to evaluate the value of all potential splits\n",
    "- Apply the criterion iteratively (greedy approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Splitting criterion: entropy\n",
    "<center><img src='fig/decision_tree/0007.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Applying the entropy criterion - split by alcoholic mother\n",
    "<center><img src='fig/decision_tree/0008.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree \n",
    "\n",
    "Applying the entropy criterion - split by place of birth\n",
    "<center><img src='fig/decision_tree/0009.jpg' alt=\"Drawing\" style=\"width: 1000px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree - a generalization\n",
    "\n",
    "Generally, we estimate trees using the Classification And Regression Tree (**CART**) approach:\n",
    "\n",
    "- At each step evaluate parent (before split) against children (after split)\n",
    "\n",
    "  - Can use other measures for deciding to split, e.g. gini impurity, variation (sum square pairwise over observations)\n",
    "  - When used for regression requires different criterion, e.g. variation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree - a generalization\n",
    "\n",
    "Some terminology of a CART:\n",
    "\n",
    "<center><img src='fig/decision_tree/CART_general.png' alt=\"Drawing\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree - a generalization\n",
    "\n",
    "What are some of the properties of the CART?\n",
    "- Main **advantage** has no *underfitting*, can universally approximate\n",
    "- Main **disadvantage** is high *overfitting* \n",
    "- We can trade-off the two by tuning hyperparameters, e.g. maximal depth\n",
    "  - Another solution is to grow forests rather trees (next topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree - python demonstration\n",
    "\n",
    "We now load the Decision Tree classification model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5_nearest_neighbors': 0.737, 'deciscion_tree': 0.81}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train,y_train)\n",
    "auc_scores['deciscion_tree'] = round(roc_auc_score(y_test, clf_dt.predict_proba(X_test)[:, 1]),3)\n",
    "auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble learning\n",
    "\n",
    "### *Leveraging wisdom of the crowd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "Main idea\n",
    "- Create and train many supervised models \n",
    "\n",
    "How is this implemented?\n",
    "- Aggregation of independent models (e.g. bagging)\n",
    "  - E.g. `mode` in classification and `mean` in regression\n",
    "  - Each model is weighted equally in aggregation\n",
    "- Sequential improvement of models (e.g. boosting)\n",
    "  - Next model learn by current model's errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "One way to implement model aggregation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is it?\n",
    "  - Acronym for **B**ootstrap **Agg**regate **'ing**\n",
    "  - I.e. repeatedly estimate model on bootstrap samples\n",
    "  \n",
    "- Why should we use it?\n",
    "  - Advantage: Decreases overfitting, implies better generalization performance\n",
    "  - Disadvantage: Takes a longer to train/estimate model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "A quick reminder of **bootstrap**:\n",
    "\n",
    "- Given a table of length $n$, randomly select $n$ rows with replacement $B$ times\n",
    "\n",
    "Example with five draws from dimensional array, $X=[0, 1, 2, 3, 4,5]$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 0, 3, 3, 3],\n",
       "       [1, 3, 5, 2, 4, 0],\n",
       "       [0, 4, 2, 1, 0, 1],\n",
       "       [5, 1, 5, 0, 1, 4],\n",
       "       [3, 0, 3, 5, 0, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n,B = 6, 5\n",
    "X = np.arange(n)\n",
    "bootstrap = np.random.RandomState(0).choice(X,size=[B,n])\n",
    "bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging \n",
    "\n",
    "\n",
    "Example of bagging algorithm. Assume we have training data set with features $X$ and target $Y$:\n",
    "  - For $b = 1, ..., B$:\n",
    "    1. Draw bootstrap sample from training data $X, Y$; denote $X_b, Y_b$.\n",
    "    2. Train a classification or regression model $f_b$ on $X_b, Y_b$.\n",
    "\n",
    "- After training, predictions for unseen samples $x'$ can be made by averaging the predictions from all the individual classification on $x'$:\n",
    "\n",
    "\n",
    "$$\\hat{f}=\\frac{1}{B}\\sum_{b=1}^{B}f_b(x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest\n",
    "\n",
    "\n",
    "*Random forest* is a  popular machine learning method that uses bagging by design:\n",
    "\n",
    "\n",
    "- Train/estimate the forest:\n",
    "    1. Draw $B$ bootstrap samples on training data, one for each CART\n",
    "    1. Grow a decision tree from the bootstrap sample. At each node:\n",
    "      - Randomly select $d$ features without replacement.\n",
    "- Apply the forest: \n",
    "   - For each observation we aggregate predictions from CARTs \n",
    "\n",
    "NOTE: bootstrapping from rows and subsampling from columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest\n",
    "\n",
    "How does a random forest look work? Example of $N=B$ trees:\n",
    "\n",
    "<center><img src='fig/RF.png' alt=\"Drawing\" style=\"width: 1100px;\"/></center>\n",
    "\n",
    "*(by [Machado et al. 2015](http://dx.doi.org/10.1186/s13567-015-0219-7) licensed under [CC-BY](https://www.researchgate.net/figure/Random-forest-model-Example-of-training-and-classification-processes-using-random_fig5_280533599))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest\n",
    "\n",
    "\n",
    "Why do we subsample feature? \n",
    "  - Reduces overfitting by making trees more independent - key part of the procedure.\n",
    "\n",
    "\n",
    "Random forest is nice to use - off the shelf and generally good performance, however, not transparent\n",
    "\n",
    "- When fitting CARTs, we estimate the information gain for every feature at each split\n",
    "- We can summarize the *importance* of a feature as its relative amount of information gain delivered during classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forest\n",
    "\n",
    "We now load the Decision Tree classification model and apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5_nearest_neighbors': 0.737, 'deciscion_tree': 0.81, 'random_forest': 0.853}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train,y_train)\n",
    "auc_scores['random_forest'] = round(roc_auc_score(y_test, clf_rf.predict_proba(X_test)[:, 1]), 3)\n",
    "auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "*Social learning*\n",
    "\n",
    "- Background\n",
    "  - [Kearns (1988)](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf) posed the fundamental question: how can we turn weak learners (not universal approximators) into strong learners?\n",
    "- Answer to Kearns' problem came by the following procedure:\n",
    "  - Train models repeatedly \n",
    "  - For each new model set weights to focus on previous models' errors.\n",
    "  \n",
    "- Intuition: Many weak models that learn from each other’s mistakes, combine into one strong model\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "How can we implement boosting?\n",
    "- ***Ada***<text><text>*ptive* ***Boost***: Sequentially update training weights - upweigh with previous errors of models \n",
    "- ***Gradient***<text><text> ***Boost***: Sequentially train on previous errors of models - gradient approach\n",
    "\n",
    "Why should we use these methods - often works even better than random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Implementation of adaptive boosting: \n",
    "\n",
    "1. Create a weight vector $w$ that encodes the importance of each training sample\n",
    "2. For $j$ out of $m$ boosting iterations:\n",
    "  1. Train a weighted weak classifier train (e.g. decision tree with max depth 10)\n",
    "  1. Predict class labels\n",
    "  1. Update $w$ based on the errors that makes (steps *c* to *f* in Raschka page 248)\n",
    "3. To make predictions apply weighted voting, i.e. giving more prediction weight to less error prone\n",
    "classifiers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks and deep learning\n",
    "\n",
    "### Models inspired by biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural background\n",
    "\n",
    "What happens when we combine multiple neurons? Network of neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='https://www.publicdomainpictures.net/pictures/380000/velka/kunstliche-intelligenz-201103.jpg' alt=\"Drawing\" style=\"width: 680px;\"/></center>\n",
    "\n",
    "(artist: [Gerd Altmann](https://www.publicdomainpictures.net/en/view-image.php?image=372933&picture=artificial-intelligence-201103), CC-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural background\n",
    "\n",
    "All the linear and logistic models we saw can be expressed in the following form:\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch12/images/12_01.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n",
    "\n",
    "(Image from [Raschka, 2017](https://github.com/rasbt/python-machine-learning-book-2nd-edition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural background\n",
    "\n",
    "Recall: logistic regression was one of the simple linear models / neurons\n",
    "\n",
    "How does the perform on the titanic data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5_nearest_neighbors': 0.737,\n",
       " 'deciscion_tree': 0.81,\n",
       " 'random_forest': 0.853,\n",
       " 'random_log_reg': 0.84}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(max_iter=100000)\n",
    "clf_lr.fit(X_train,y_train)\n",
    "auc_scores['random_log_reg'] = round(roc_auc_score(y_test, clf_lr.predict_proba(X_test)[:, 1]), 3)\n",
    "auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural example\n",
    "\n",
    "What about slightly more advanced data? \n",
    "- Suppose we have circular data: \n",
    "  - $y$ is <font style=\"color:blue;\">blue</font> if $x$ near origin; \n",
    "  - otherwise, $y$ is <font style=\"color:red;\">red</font>. \n",
    "- Let $P(y=red)=0.7$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural example\n",
    "\n",
    "How does the model data look like before seeing $x$?\n",
    "\n",
    "<center><img src='fig/neural/circle_even_likelihood.png' alt=\"Drawing\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural example\n",
    "\n",
    "What if we depict the predictions of logistic regression?\n",
    "\n",
    "<center><img src='fig/neural/circle_lr_likelihood.png' alt=\"Drawing\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural example\n",
    "\n",
    "Could we somehow create intermediate features that help us model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We create four logistic regressions, two horizontal (left) and two vertical (right)\n",
    "- none of them are accurate, what about together?\n",
    "<center><img src='fig/neural/circle_direction_likelihood.png' alt=\"Drawing\" style=\"width: 1200px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural example\n",
    "\n",
    "When we combine the four models, taking the maximal value of $P(y=red)$:\n",
    "\n",
    "<center><img src='fig/neural/circle_combine_direction_likelihood.png' alt=\"Drawing\" style=\"width: 700px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "What happened in the previous example? Manually created a neural network. \n",
    "- General structure of an (artificial) neural network, 1 hidden layer:\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch12/images/12_02.png' alt=\"Drawing\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "(Image from [Raschka, 2017](https://github.com/rasbt/python-machine-learning-book-2nd-edition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "Can we compare the neural network with a single neuron?\n",
    "- Yes, the neuron has no hidden layers.\n",
    "- But what are the intermediate/hidden layer?\n",
    "  - They consist of $h$ extra auxiliary models that are used to predict the output. \n",
    "\n",
    "Why might neural architecture be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "Fundamental results in 80's:\n",
    "\n",
    "- Methods for estimating neural networks were developed \n",
    "- [Cybenko (1989)](https://doi.org/10.1007%2FBF02551274) and [Hornik (1991)](https://doi.org/10.1016%2F0893-6080%2891%2990009-T) demonstrate that neural networks with a **single hidden layer** can universally approximate for $h\\rightarrow\\infty$\n",
    "\n",
    "  \n",
    "Is this important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Not at the time - models e.g. non-linear SVM, random forest could be trained with less power for similar accuracy.\n",
    "- Consequence: popularity of neural networks fell in 1990's and early 2000's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "Today neural networks have changed computing, e.g. to infer content of images:\n",
    "\n",
    "<br>\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg/1200px-Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n",
    "\n",
    "(author: [MTheiler](https://commons.wikimedia.org/wiki/User:MTheiler), license: CC-BY-SA 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "What happend since the early 2000's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Break-through in 2008-12 driven by three factors\n",
    "- Data not abundant and accessible as today\n",
    "- Computers not powerful enough, in particular number of cores \n",
    "  - .. solved by modern Graphical Processing Units (GPU) for gaming\n",
    "- Changes in architecture: \n",
    "  - Multiple layers rather than 1 hidden layer, known as ***deep learning***\n",
    "  - Many more advances, e.g. ***convolution*** of layers that divides input into independent parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mechanics of neural networks \n",
    "\n",
    "Model works with **feed-forward**: start with input, then proceed through layers\n",
    "\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch12/images/12_02.png' alt=\"Drawing\" style=\"width: 750px;\"/></center>\n",
    "\n",
    "(Image from [Raschka, 2017](https://github.com/rasbt/python-machine-learning-book-2nd-edition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mechanics of neural networks \n",
    "\n",
    "The models in the both layers consist of linear models with activation, $\\phi$:\n",
    "\n",
    "\\begin{align}\n",
    "a_k^{(hid)} &= \\phi\\Big(\\sum_{i=0}^m a_i^{(in)}\\cdot w_{i,k}^{(hid)}\\Big), \\quad k\\in\\{1,..,h\\} \\\\\n",
    "a_l^{(out)} &= \\phi\\Big(\\sum_{i=0}^h a_i^{(hid)}\\cdot w_{i,l}^{(out)}\\Big), \\quad l\\in\\{1,..,t\\} \\\\\n",
    "            &= \\phi\\left(\\sum_{i=0}^h \\phi\\Big(\\sum_{j=0}^m a_j^{(in)}\\cdot w_{j,k}^{(hid)}\\Big)\\cdot w_{i,l}^{(out)}\\right), \\quad k\\in\\{1,..,h\\}, l\\in\\{1,..,t\\} \\\\\n",
    "\\end{align}\n",
    "\n",
    "What happens if $\\phi$ is linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mechanics of neural networks \n",
    "\n",
    "Estimation of neural network is possible using ***backpropagation***\n",
    "\n",
    "- Idea, use chain rule: $\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x}$ where $y=f(u)$, $u=g(x)$ (Leibniz notation)\n",
    "- Fix initial parameters drawn randomly\n",
    "- Apply gradient descent repeatedly for a given number of epochs as follows\n",
    "  1. Compute the errors in output stage using feed-forward\n",
    "  1. Optimize model parameters of output layer given output errors\n",
    "  1. Optimize model parameters of hidden layer activations given using chain rule \n",
    "    - (keeping parameters of output layer fixed)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mechanics of neural networks \n",
    "\n",
    "Drawbacks of neural networks\n",
    "\n",
    "- Minimal transparency of how input affects output, discrimination?\n",
    "- Optimization problem is non-linear\n",
    "\n",
    "<br>\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/1280px-Extrema_example.svg.png' alt=\"Drawing\" style=\"width: 450px;\"/></center>\n",
    "\n",
    "(By *KSmrq*, licensed under CC-BY-SA 3.0, [source](https://commons.wikimedia.org/w/index.php?title=User:KSmrq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appliction in economics\n",
    "\n",
    "Methodological\n",
    "- [Hartford et al. (2017)](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf) develop framework for using neural networks in instrumental variables to parse non-linear, high dimensional treatment \n",
    "  - show application to treatment of URL shown in search advertisement\n",
    "- [Farrell, Liang, Misra (2021)](https://doi.org/10.3982/ECTA16901) develop a generalized framework for using neural networks in estimation and inference\n",
    "- [Chernozhukov et al. (2017).](https://doi.org/10.1257/aer.p20171038), [Chernozhukov et al. (2018).](https://doi.org/10.1111/ectj.12097) develops a two step framework called Double Machine Learning to apply machine learning in estimation.\n",
    "  - Allows for applications of machine learning \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appliction in economics\n",
    "\n",
    "Data imputation\n",
    "- Input: Google Street View (360° images)\n",
    "  - Infer neighborhood safety ([Naik, Raskar & Hidalgo, 2016](https://doi.org/10.1257/aer.p20161030))\n",
    "  - To measure and track development in sociodemographic compositon ([Gebru et al. 2017](https://doi.org/10.1073/pnas.1700035114); [Nikhil et al. 2017](https://doi.org/10.1073/pnas.1619003114))\n",
    "- [Layout Parser](https://layout-parser.github.io/),  which can automatically parse the structure of historical records [Zeiyang et al. (2021)](https://arxiv.org/abs/2103.15348)\n",
    "  - M. Wust and C.M. Dahl use techniques to extract information about health visitors in 60's\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appliction in economics\n",
    "\n",
    "Reinforcement learning\n",
    "- Dynamic structural models that are solved approximately.\n",
    "- Work in progress by colleagues in Copenhagen: Estimate choice models using neural networks. Can increase state space drastically!\n",
    "-  [Calvano et al. 2020](https://doi.org/10.1257/aer.20190623) demonstrate that pricing algorithms implicitly collude (i.e. coordinate on price setting) even when optimized own pricing\n",
    "  - How should we regulate price setting online? (and offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Let's to fit the network with 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"../base/data/mnist/mnist.pkl\",'rb') as f:\n",
    "    mnist = pickle.load(f)\n",
    "X_train,y_train,X_test,y_test = \\\n",
    "    mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "We load the code from Raschka chapter 12. Available in this [auxiliary notebook](neural_network_auxiliary.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnet import NeuralNetMLP\n",
    "\n",
    "clf_nn = {}\n",
    "for epoch in [2, 20, 200]:\n",
    "    clf_nn[epoch] = \\\n",
    "        NeuralNetMLP(n_hidden=100,\n",
    "                     l2=0.01,\n",
    "                     epochs=epoch,\n",
    "                     eta=0.0005,\n",
    "                     minibatch_size=100,\n",
    "                     shuffle=True,\n",
    "                     seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Let's to fit the network with 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/2 | Cost: 44472.59 | Train/Valid Acc.: 88.74%/91.50% "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neuralnet.NeuralNetMLP at 0x7fdee89b4f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn[2].fit(X_train=X_train[:55000],\n",
    "              y_train=y_train[:55000],\n",
    "              X_valid=X_train[55000:],\n",
    "              y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Let's to fit the network with 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/20 | Cost: 26911.76 | Train/Valid Acc.: 92.89%/94.60% "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neuralnet.NeuralNetMLP at 0x7fdefb7c6670>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn[20].fit(X_train=X_train[:55000],\n",
    "               y_train=y_train[:55000],\n",
    "               X_valid=X_train[55000:],\n",
    "               y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Let's to fit the network with 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200/200 | Cost: 14340.23 | Train/Valid Acc.: 96.47%/96.80% "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<neuralnet.NeuralNetMLP at 0x7fdefb7c6340>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn[200].fit(X_train=X_train[:55000],\n",
    "                y_train=y_train[:55000],\n",
    "                X_valid=X_train[55000:],\n",
    "                y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "What happened to errors?\n",
    "- Almost halved for ten fold increase in epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outro supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outro supervised ML\n",
    "\n",
    "We have seen that sacrificing 'unbiased' property, we get much stronger models\n",
    "- General tradeoff: overfitting/underfitting\n",
    "- Issue: non-linear models are less ***transparent***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outro - non-linear ML\n",
    "\n",
    "Tree based models\n",
    "- Simple, in particular decision tree\n",
    "- Ensemble version performed best\n",
    "- Later in course we extend to allow for estimating causal effects (causal forest and related models)\n",
    "\n",
    "Kernel/support vector machines\n",
    "- Less common nowadays, replaced by neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outro - non-linear ML\n",
    "\n",
    "Very big potential for research of neural networks\n",
    "- Understanding causal effects of non-linear data, e.g.\n",
    "  - Visual and audible input\n",
    "  - Human life trajectory\n",
    "- Understanding consequences of algorithms \n",
    "- Potentially for estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outro - non-linear ML\n",
    "\n",
    "We have only scratched the surface of neural networks\n",
    "- Architectures can include:\n",
    "  - Temporal structure, convolution, regularization, activation\n",
    "- Deep learning is half engineering/half science \n",
    "\n",
    "The world is your oyster\n",
    "- Deep learning courses are available online and in most universities\n",
    "- Two big frameworks: pyTorch or TensorFlow\n",
    "  - May want to invest in GPU!\n",
    "- Melissa Dell from Harvard has a course called, *Unleashing Novel Data at Scale*, see here for the [reading list](https://dell-research-harvard.github.io/teaching/economics-2355) that may be of interest"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
